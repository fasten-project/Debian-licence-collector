{
    "content": [
        {
            "hidden": false,
            "name": "__init__.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 3306,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "activation.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 4537,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "batchnorm.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 2468,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "conv.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 33600,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "embedding_ops.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 11130,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "functional_modules.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 8269,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "linear.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 11547,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "normalization.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 6227,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        },
        {
            "hidden": false,
            "name": "utils.py",
            "stat": {
                "perms": "rw-r--r--",
                "size": 2388,
                "symlink_dest": null,
                "type": "-"
            },
            "type": "file"
        }
    ],
    "directory": "modules",
    "package": "pytorch",
    "path": "pytorch/1.8.1-2/torch/nn/quantized/modules",
    "pkg_infos": {
        "area": "main",
        "copyright": true,
        "ctags_count": 0,
        "license": "/copyright/license/pytorch/1.8.1-2/",
        "metric": {
            "size": 90236
        },
        "pts_link": "https://tracker.debian.org/pkg/pytorch",
        "sloc": [
            [
                "cpp",
                763893
            ],
            [
                "python",
                395591
            ],
            [
                "ansic",
                72939
            ],
            [
                "asm",
                8003
            ],
            [
                "sh",
                2912
            ],
            [
                "java",
                2895
            ],
            [
                "xml",
                268
            ],
            [
                "makefile",
                258
            ],
            [
                "ruby",
                148
            ],
            [
                "yacc",
                144
            ],
            [
                "objc",
                51
            ],
            [
                "lex",
                44
            ]
        ],
        "suites": [
            "sid"
        ],
        "vcs_browser": "https://salsa.debian.org/deeplearning-team/pytorch",
        "vcs_type": "git"
    },
    "type": "directory",
    "version": "1.8.1-2"
}